{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('KAGGLE_JSON')"
      ],
      "metadata": {
        "id": "XknQycrEU9hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load Kaggle credentials from Colab secrets\n",
        "kaggle_json_content = userdata.get('KAGGLE_JSON')\n",
        "\n",
        "kaggle_config_dir = os.path.join(os.path.expanduser('~'), '.config', 'kaggle')\n",
        "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(kaggle_config_dir, 'kaggle.json'), 'w') as f:\n",
        "    f.write(kaggle_json_content)\n",
        "\n",
        "os.chmod(os.path.join(kaggle_config_dir, 'kaggle.json'), 0o600)\n",
        "\n",
        "print(f\"Kaggle credentials loaded to {kaggle_config_dir}.\")"
      ],
      "metadata": {
        "id": "jbpQvgENSM_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "kaggle_file_path = os.path.join(os.path.expanduser('~'), '.kaggle', 'kaggle.json')\n",
        "\n",
        "if os.path.exists(kaggle_file_path):\n",
        "    print(f\"kaggle.json found at: {kaggle_file_path}\")\n",
        "else:\n",
        "    print(f\"kaggle.json NOT found at: {kaggle_file_path}\")"
      ],
      "metadata": {
        "id": "8YrjoZKZVRAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qV8JtI5Qquq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kaggle\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class AtlasDatasetDownloader:\n",
        "\n",
        "\n",
        "    def __init__(self, data_dir='data/'):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.raw_dir = self.data_dir / 'raw'\n",
        "        self.raw_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def download_from_kaggle(self):\n",
        "\n",
        "        print(\"Downloading Atlas dataset from Kaggle...\")\n",
        "\n",
        "        # Download dataset\n",
        "        kaggle.api.dataset_download_files(\n",
        "            'silverstone1903/atlas-e-commerce-clothing-product-categorization',\n",
        "            path=str(self.raw_dir),\n",
        "            unzip=True\n",
        "        )\n",
        "\n",
        "        print(f\"Dataset downloaded to {self.raw_dir}\")\n",
        "\n",
        "    def load_metadata(self):\n",
        "\n",
        "        csv_path = self.raw_dir / 'atlas_data.csv'\n",
        "\n",
        "        if not csv_path.exists():\n",
        "            # Try feather format\n",
        "            csv_path = self.raw_dir / 'atlas_data.feather'\n",
        "            df = pd.read_feather(csv_path)\n",
        "        else:\n",
        "            df = pd.read_csv(csv_path)\n",
        "\n",
        "        print(f\"Loaded {len(df)} image records\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def verify_dataset(self, df):\n",
        "\n",
        "        images_dir = self.raw_dir / 'images'\n",
        "\n",
        "        if not images_dir.exists():\n",
        "            raise FileNotFoundError(f\"Images directory not found at {images_dir}\")\n",
        "\n",
        "        # Count available images\n",
        "        image_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png'))\n",
        "        print(f\"Found {len(image_files)} image files\")\n",
        "\n",
        "        # Check for missing images\n",
        "        missing_count = 0\n",
        "        # Assuming 'item_ID' column contains the image filenames\n",
        "        image_column_name = 'item_ID'\n",
        "\n",
        "        if image_column_name not in df.columns:\n",
        "             raise ValueError(f\"Column '{image_column_name}' not found in the DataFrame.\")\n",
        "\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            img_path = images_dir / row[image_column_name]\n",
        "            if not img_path.exists():\n",
        "                missing_count += 1\n",
        "\n",
        "        print(f\"Missing images: {missing_count}/{len(df)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    downloader = AtlasDatasetDownloader()\n",
        "    downloader.download_from_kaggle()\n",
        "    df = downloader.load_metadata()\n",
        "    df = downloader.verify_dataset(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Analysis"
      ],
      "metadata": {
        "id": "JlT6e4oe0PDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "class AtlasDatasetAnalyzer:\n",
        "    \"\"\"Analyze Atlas dataset structure and statistics\"\"\"\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def parse_taxonomy(self):\n",
        "        \"\"\"Parse hierarchical category paths\"\"\"\n",
        "        # Create the 'category_path' column by combining existing columns\n",
        "        self.df['category_path'] = self.df['gender'] + ' > ' + self.df['category'] + ' > ' + self.df['sub-category']\n",
        "\n",
        "        # Category path format: Gender > ClothingType > SpecificCategory\n",
        "        self.df['level1'] = self.df['category_path'].apply(lambda x: x.split(' > ')[0] if isinstance(x, str) and len(x.split(' > ')) > 0 else None)\n",
        "        self.df['level2'] = self.df['category_path'].apply(lambda x: x.split(' > ')[1] if isinstance(x, str) and len(x.split(' > ')) > 1 else None)\n",
        "        self.df['level3'] = self.df['category_path'].apply(lambda x: x.split(' > ')[2] if isinstance(x, str) and len(x.split(' > ')) > 2 else None)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def analyze_distribution(self):\n",
        "        \"\"\"Analyze category distribution\"\"\"\n",
        "        print(\"\\n=== Level 1 (Gender) Distribution ===\")\n",
        "        print(self.df['level1'].value_counts())\n",
        "\n",
        "        print(\"\\n=== Level 2 (Clothing Type) Distribution ===\")\n",
        "        print(self.df['level2'].value_counts())\n",
        "\n",
        "        print(\"\\n=== Level 3 (Specific Category) Distribution ===\")\n",
        "        print(self.df['level3'].value_counts().head(20))\n",
        "\n",
        "        print(f\"\\nTotal unique category paths: {self.df['category_path'].nunique()}\")\n",
        "\n",
        "    def visualize_distribution(self):\n",
        "        \"\"\"Visualize category distributions\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Level 1 distribution\n",
        "        level1_counts = self.df['level1'].value_counts()\n",
        "        axes[0, 0].bar(level1_counts.index, level1_counts.values, color='steelblue')\n",
        "        axes[0, 0].set_title('Level 1 (Gender) Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Gender')\n",
        "        axes[0, 0].set_ylabel('Count')\n",
        "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Level 2 distribution\n",
        "        level2_counts = self.df['level2'].value_counts().head(10)\n",
        "        axes[0, 1].barh(level2_counts.index, level2_counts.values, color='coral')\n",
        "        axes[0, 1].set_title('Top 10 Level 2 (Clothing Type) Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Count')\n",
        "\n",
        "        # Level 3 distribution (top 20)\n",
        "        level3_counts = self.df['level3'].value_counts().head(20)\n",
        "        axes[1, 0].barh(level3_counts.index, level3_counts.values, color='mediumseagreen')\n",
        "        axes[1, 0].set_title('Top 20 Level 3 (Specific Category) Distribution', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Count')\n",
        "        axes[1, 0].invert_yaxis()\n",
        "\n",
        "        # Category path distribution (top 15)\n",
        "        path_counts = self.df['category_path'].value_counts().head(15)\n",
        "        axes[1, 1].barh(range(len(path_counts)), path_counts.values, color='mediumpurple')\n",
        "        axes[1, 1].set_yticks(range(len(path_counts)))\n",
        "        axes[1, 1].set_yticklabels([p[:40] + '...' if len(p) > 40 else p for p in path_counts.index], fontsize=8)\n",
        "        axes[1, 1].set_title('Top 15 Complete Category Paths', fontsize=14, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Count')\n",
        "        axes[1, 1].invert_yaxis()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('data/category_distribution.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def check_class_imbalance(self):\n",
        "        \"\"\"Check for class imbalance issues\"\"\"\n",
        "        category_counts = self.df['category_path'].value_counts()\n",
        "\n",
        "        print(\"\\n=== Class Imbalance Analysis ===\")\n",
        "        print(f\"Most frequent category: {category_counts.iloc[0]} samples\")\n",
        "        print(f\"Least frequent category: {category_counts.iloc[-1]} samples\")\n",
        "        print(f\"Imbalance ratio: {category_counts.iloc[0] / category_counts.iloc[-1]:.2f}:1\")\n",
        "\n",
        "        # Categories with < 100 samples\n",
        "        rare_categories = category_counts[category_counts < 100]\n",
        "        print(f\"\\nCategories with < 100 samples: {len(rare_categories)}\")\n",
        "\n",
        "        return category_counts\n",
        "\n",
        "# Usage\n",
        "df = pd.read_csv('data/raw/atlas_data.csv')\n",
        "analyzer = AtlasDatasetAnalyzer(df)\n",
        "df = analyzer.parse_taxonomy()\n",
        "analyzer.analyze_distribution()\n",
        "analyzer.visualize_distribution()\n",
        "category_counts = analyzer.check_class_imbalance()"
      ],
      "metadata": {
        "id": "W6mn6Xz5Q2ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data Preprocessing and Augmentation"
      ],
      "metadata": {
        "id": "dwBGSapa0GyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/data/preprocessing.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "class AtlasImagePreprocessor:\n",
        "\n",
        "\n",
        "    def __init__(self, target_size=(224, 224), normalize=True):\n",
        "        self.target_size = target_size\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # ImageNet statistics for transfer learning\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        return img\n",
        "\n",
        "    def resize_image(self, img):\n",
        "\n",
        "        # Calculate scaling factor\n",
        "        width, height = img.size\n",
        "        scale = min(self.target_size[0] / width, self.target_size[1] / height)\n",
        "\n",
        "        new_width = int(width * scale)\n",
        "        new_height = int(height * scale)\n",
        "\n",
        "        # Resize\n",
        "        img = img.resize((new_width, new_height), Image.LANCZOS)\n",
        "\n",
        "        # Create padded image\n",
        "        padded_img = Image.new('RGB', self.target_size, (255, 255, 255))\n",
        "        paste_x = (self.target_size[0] - new_width) // 2\n",
        "        paste_y = (self.target_size[1] - new_height) // 2\n",
        "        padded_img.paste(img, (paste_x, paste_y))\n",
        "\n",
        "        return padded_img\n",
        "\n",
        "    def normalize_image(self, img):\n",
        "\n",
        "        img_array = np.array(img).astype(np.float32) / 255.0\n",
        "\n",
        "        if self.normalize:\n",
        "            img_array = (img_array - self.mean) / self.std\n",
        "\n",
        "        return img_array\n",
        "\n",
        "    def preprocess(self, image_path):\n",
        "\n",
        "        img = self.load_image(image_path)\n",
        "        img = self.resize_image(img)\n",
        "        img_array = self.normalize_image(img)\n",
        "\n",
        "        # Convert to tensor (C, H, W)\n",
        "        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)\n",
        "\n",
        "        return img_tensor\n",
        "\n",
        "# Data augmentation for training\n",
        "class AtlasAugmentation:\n",
        "\n",
        "\n",
        "    def __init__(self, target_size=(224, 224)):\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.RandomCrop(target_size),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.val_transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(target_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def get_train_transform(self):\n",
        "        return self.train_transform\n",
        "\n",
        "    def get_val_transform(self):\n",
        "        return self.val_transform\n"
      ],
      "metadata": {
        "id": "xbQSgRSZXCi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ca6a92c"
      },
      "source": [
        "!pip install opencv-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image # Import Image here\n",
        "\n",
        "class AtlasDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, df, images_dir, transform=None, target_type='path'):\n",
        "\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.transform = transform\n",
        "        self.target_type = target_type\n",
        "\n",
        "        # Create label encodings\n",
        "        if target_type == 'path':\n",
        "            self.classes = sorted(df['category_path'].unique())\n",
        "        else:\n",
        "            self.classes = sorted(df['level3'].unique())\n",
        "\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        # Use 'item_ID' column for image filename and append extension\n",
        "        img_filename = row['item_ID'] + '.jpg' # Assuming .jpg extension\n",
        "        img_path = self.images_dir / img_filename\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Get label\n",
        "        if self.target_type == 'path':\n",
        "            label = self.class_to_idx[row['category_path']]\n",
        "        else:\n",
        "            label = self.class_to_idx[row['level3']]\n",
        "\n",
        "        return img, label\n",
        "\n",
        "def create_data_splits(df, test_size=0.30, val_size=0.05, random_state=42):\n",
        "\n",
        "    # First split: train+val vs test\n",
        "    train_val_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        stratify=df['category_path'],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Second split: train vs val\n",
        "    val_ratio = val_size / (1 - test_size)\n",
        "    train_df, val_df = train_test_split(\n",
        "        train_val_df,\n",
        "        test_size=val_ratio,\n",
        "        stratify=train_val_df['category_path'],\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Val set: {len(val_df)} samples ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Save splits\n",
        "def save_splits(train_df, val_df, test_df, output_dir='data/splits'):\n",
        "    \"\"\"Save data splits to CSV files\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_df.to_csv(output_dir / 'train.csv', index=False)\n",
        "    val_df.to_csv(output_dir / 'val.csv', index=False)\n",
        "    test_df.to_csv(output_dir / 'test.csv', index=False)\n",
        "\n",
        "    print(f\"Splits saved to {output_dir}\")"
      ],
      "metadata": {
        "id": "AvPuUUBrXW-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: ResNet34 Image Classifier**"
      ],
      "metadata": {
        "id": "TvaEFTLWzs_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/models/resnet_classifier.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class ResNet34Classifier(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes=52, pretrained=True, dropout=0.5):\n",
        "        super(ResNet34Classifier, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet34\n",
        "        self.resnet = models.resnet34(pretrained=pretrained)\n",
        "\n",
        "        # Get number of features from last layer\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        # Replace final fully connected layer\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "    def freeze_backbone(self):\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze final layer\n",
        "        for param in self.resnet.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def unfreeze_backbone(self):\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# Usage\n",
        "model = ResNet34Classifier(num_classes=52, pretrained=True)\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ],
      "metadata": {
        "id": "mw2iCZPMXg5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: Attention-Based Sequence-to-Sequence Model**"
      ],
      "metadata": {
        "id": "duaFQTS8zTXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        resnet = models.resnet101(pretrained=True)\n",
        "\n",
        "        # Remove linear and pool layers\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Adaptive pooling to fixed spatial size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        # Fine-tune only top layers\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Fine-tune top 2 blocks\n",
        "        if fine_tune:\n",
        "            for c in list(self.resnet.children())[5:]:\n",
        "                for p in c.parameters():\n",
        "                    p.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort by decreasing caption length\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)\n",
        "\n",
        "        # Decode lengths\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors for predictions and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n",
        "\n",
        "        # Decode step by step\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t])\n",
        "            )\n",
        "            preds = self.fc(self.dropout_layer(h))\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n",
        "\n",
        "\n",
        "\n",
        "class Seq2SeqModel(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, images, captions, caption_lengths):\n",
        "        encoder_out = self.encoder(images)\n",
        "        predictions, encoded_captions, decode_lengths, alphas, sort_ind = self.decoder(\n",
        "            encoder_out, captions, caption_lengths\n",
        "        )\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n"
      ],
      "metadata": {
        "id": "VicmMxHBXpeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "722d7d87"
      },
      "source": [
        "import os\n",
        "\n",
        "config_dir = 'configs'\n",
        "config_filename = 'training_config.yaml'\n",
        "config_path = os.path.join(config_dir, config_filename)\n",
        "\n",
        "# Create the configs directory if it doesn't exist\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "\n",
        "# YAML content as a string\n",
        "yaml_content = \"\"\"# configs/training_config.yaml\n",
        "model_type: 'resnet34'  # or 'seq2seq'\n",
        "\n",
        "# Data configuration\n",
        "data:\n",
        "  images_dir: 'data/raw/images'\n",
        "  train_csv: 'data/splits/train.csv'\n",
        "  val_csv: 'data/splits/val.csv'\n",
        "  test_csv: 'data/splits/test.csv'\n",
        "  image_size: 224\n",
        "  num_workers: 4\n",
        "\n",
        "# Model configuration\n",
        "model:\n",
        "  num_classes: 52\n",
        "  pretrained: true\n",
        "  dropout: 0.5\n",
        "\n",
        "# Training hyperparameters\n",
        "training:\n",
        "  batch_size: 64\n",
        "  epochs: 100 # Increased epochs\n",
        "  learning_rate: 0.001\n",
        "  weight_decay: 0.0001\n",
        "  optimizer: 'adam'  # or 'sgd'\n",
        "  scheduler: 'reduce_on_plateau'\n",
        "  patience: 10 # Increased patience\n",
        "\n",
        "# Seq2Seq specific\n",
        "seq2seq:\n",
        "  encoder_lr: 0.0001\n",
        "  decoder_lr: 0.0004\n",
        "  attention_dim: 512\n",
        "  embed_dim: 512\n",
        "  decoder_dim: 512\n",
        "  grad_clip: 5.0\n",
        "  alpha_c: 1.0  # regularization for doubly stochastic attention\n",
        "  beam_width: 5\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to the file\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(f\"Created {config_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dd053f6"
      },
      "source": [
        "import pandas as pd\n",
        "# from src.data.dataset import create_data_splits, save_splits # Assuming AtlasDataset and other related functions are in src.data.dataset\n",
        "\n",
        "# Load the full dataset (assuming it's available from a previous step)\n",
        "# If df is not available in the environment, you might need to reload it:\n",
        "try:\n",
        "    df\n",
        "except NameError:\n",
        "    print(\"Loading data from data/raw/atlas_data.csv...\")\n",
        "    df = pd.read_csv('data/raw/atlas_data.csv')\n",
        "\n",
        "# df already has the 'category_path' column from previous analysis\n",
        "if 'category_path' not in df.columns:\n",
        "    print(\"Creating 'category_path' column...\")\n",
        "    df['category_path'] = df['gender'] + ' > ' + df['category'] + ' > ' + df['sub-category']\n",
        "\n",
        "\n",
        "# Get category counts\n",
        "category_counts = df['category_path'].value_counts()\n",
        "\n",
        "# Identify categories with only one sample\n",
        "single_sample_categories = category_counts[category_counts == 1].index\n",
        "\n",
        "# Filter out rows belonging to single-sample categories\n",
        "df_filtered = df[~df['category_path'].isin(single_sample_categories)].copy()\n",
        "\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Number of categories with a single sample: {len(single_sample_categories)}\")\n",
        "print(f\"Filtered dataset size (after removing single-sample categories): {len(df_filtered)}\")\n",
        "\n",
        "# Use the filtered dataframe for splitting\n",
        "df_to_split = df_filtered\n",
        "\n",
        "\n",
        "\n",
        "# Create train, val, and test splits using the filtered dataframe\n",
        "train_df, val_df, test_df = create_data_splits(df_to_split)\n",
        "\n",
        "# Save the splits to CSV files\n",
        "save_splits(train_df, val_df, test_df)\n",
        "\n",
        "print(\"\\nData splits created and saved successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "images_dir = 'data/raw/images'\n",
        "if os.path.exists(images_dir):\n",
        "    print(f\"Listing files in {images_dir}:\")\n",
        "    files = os.listdir(images_dir)\n",
        "    for i, file in enumerate(files[:10]):\n",
        "        print(file)\n",
        "    if len(files) > 10:\n",
        "        print(\"...\")\n",
        "    print(f\"Total files found: {len(files)}\")\n",
        "else:\n",
        "    print(f\"Images directory not found at {images_dir}\")"
      ],
      "metadata": {
        "id": "JBW5x7UvZ82T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    train_df\n",
        "except NameError:\n",
        "    print(\"Loading train_df from data/splits/train.csv...\")\n",
        "    train_df = pd.read_csv('data/splits/train.csv')\n",
        "\n",
        "print(\"\\nFirst 10 item_ID values in train_df:\")\n",
        "print(train_df['item_ID'].head(10).tolist())"
      ],
      "metadata": {
        "id": "cDKzMj3-aAnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Script for ResNet34 Classifier**"
      ],
      "metadata": {
        "id": "eNc94Z2_zBG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "class ClassifierTrainer:\n",
        "\n",
        "\n",
        "    def __init__(self, config_path='configs/training_config.yaml'):\n",
        "        with open(config_path, 'r') as f:\n",
        "            self.config = yaml.safe_load(f)\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.setup_data()\n",
        "        self.setup_model()\n",
        "        self.setup_training()\n",
        "\n",
        "    def setup_data(self):\n",
        "\n",
        "        # Load datasets\n",
        "        train_df = pd.read_csv(self.config['data']['train_csv'])\n",
        "        val_df = pd.read_csv(self.config['data']['val_csv'])\n",
        "\n",
        "        # Create augmentation\n",
        "        augmentation = AtlasAugmentation(target_size=(self.config['data']['image_size'],\n",
        "                                                      self.config['data']['image_size']))\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = AtlasDataset(\n",
        "            train_df,\n",
        "            self.config['data']['images_dir'],\n",
        "            transform=augmentation.get_train_transform()\n",
        "        )\n",
        "\n",
        "        val_dataset = AtlasDataset(\n",
        "            val_df,\n",
        "            self.config['data']['images_dir'],\n",
        "            transform=augmentation.get_val_transform()\n",
        "        )\n",
        "\n",
        "        self.num_classes = train_dataset.classes\n",
        "        print(f\"Number of classes: {len(self.num_classes)}\")\n",
        "\n",
        "        # Create DataLoaders\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config['training']['batch_size'],\n",
        "            shuffle=True,\n",
        "            num_workers=self.config['data']['num_workers'],\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=self.config['training']['batch_size'],\n",
        "            shuffle=False,\n",
        "            num_workers=self.config['data']['num_workers'],\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "\n",
        "    def setup_model(self):\n",
        "\n",
        "        self.model = ResNet34Classifier(\n",
        "            num_classes=len(self.num_classes), # Changed from self.num_classes to len(self.num_classes)\n",
        "            pretrained=self.config['model']['pretrained'],\n",
        "            dropout=self.config['model']['dropout']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Freeze backbone initially for transfer learning\n",
        "        self.model.freeze_backbone()\n",
        "\n",
        "    def setup_training(self):\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        if self.config['training']['optimizer'] == 'adam':\n",
        "            self.optimizer = optim.Adam(\n",
        "                filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "                lr=self.config['training']['learning_rate'],\n",
        "                weight_decay=self.config['training']['weight_decay']\n",
        "            )\n",
        "        else:\n",
        "            self.optimizer = optim.SGD(\n",
        "                filter(lambda p: p.requires_grad, self.model.parameters()),\n",
        "                lr=self.config['training']['learning_rate'],\n",
        "                momentum=0.9,\n",
        "                weight_decay=self.config['training']['weight_decay']\n",
        "            )\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=self.config['training']['patience'],\n",
        "\n",
        "        )\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1} [Train]')\n",
        "        for images, labels in pbar:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Forward pass\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100.*correct/total:.2f}%'\n",
        "            })\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def validate(self, epoch):\n",
        "\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch+1} [Val]')\n",
        "            for images, labels in pbar:\n",
        "                images = images.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{100.*correct/total:.2f}%'\n",
        "                })\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        print(f\"Training on {self.device}\")\n",
        "        print(f\"Total epochs: {self.config['training']['epochs']}\")\n",
        "\n",
        "        # Create directory for model checkpoints if it doesn't exist\n",
        "        Path('models/checkpoints').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for epoch in range(self.config['training']['epochs']):\n",
        "            # Train\n",
        "            train_loss, train_acc = self.train_epoch(epoch)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc = self.validate(epoch)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config['training']['epochs']}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_acc': val_acc,\n",
        "                }, 'models/checkpoints/best_resnet34.pth')\n",
        "                print(\"✓ Saved best model\")\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.config['training']['patience'] * 2:\n",
        "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "            # Unfreeze backbone after initial epochs\n",
        "            if epoch == 10:\n",
        "                print(\"\\nUnfreezing backbone for fine-tuning...\")\n",
        "                self.model.unfreeze_backbone()\n",
        "                self.optimizer = optim.Adam(\n",
        "                    self.model.parameters(),\n",
        "                    lr=self.config['training']['learning_rate'] / 10\n",
        "                )\n",
        "\n",
        "        print(\"\\n✓ Training completed!\")\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    trainer = ClassifierTrainer()\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "LLood2QAYkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "AjvLwOYLyh7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os # Import os module\n",
        "\n",
        "class ModelEvaluator:\n",
        "\n",
        "\n",
        "    def __init__(self, model, test_loader, class_names, device):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "\n",
        "    def predict(self):\n",
        "\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(self.test_loader, desc='Predicting')\n",
        "            for images, labels in pbar:\n",
        "                images = images.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                probs = torch.softmax(outputs, dim=1)\n",
        "                _, preds = outputs.max(1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "    def calculate_metrics(self, preds, labels):\n",
        "\n",
        "        # Overall metrics\n",
        "        accuracy = (preds == labels).mean()\n",
        "\n",
        "        # Per-class metrics\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(\n",
        "            labels, preds, average=None, zero_division=0\n",
        "        )\n",
        "\n",
        "        # Micro and macro averages\n",
        "        micro_f1 = f1_score(labels, preds, average='micro')\n",
        "        macro_f1 = f1_score(labels, preds, average='macro')\n",
        "\n",
        "        print(f\"\\n=== Overall Metrics ===\")\n",
        "        print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "        print(f\"Micro F1-Score: {micro_f1:.4f}\")\n",
        "        print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'micro_f1': micro_f1,\n",
        "            'macro_f1': macro_f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'support': support\n",
        "        }\n",
        "\n",
        "    def plot_confusion_matrix(self, preds, labels, save_path='results/confusion_matrix.png'):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        # Create directory if it doesn't exist\n",
        "        output_dir = os.path.dirname(save_path)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "\n",
        "        plt.figure(figsize=(20, 18))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=False,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=self.class_names,\n",
        "            yticklabels=self.class_names,\n",
        "            cbar_kws={'label': 'Count'}\n",
        "        )\n",
        "        plt.title('Confusion Matrix - Atlas Clothing Classifier', fontsize=16, fontweight='bold')\n",
        "        plt.ylabel('True Label', fontsize=14)\n",
        "        plt.xlabel('Predicted Label', fontsize=14)\n",
        "        plt.xticks(rotation=90, fontsize=8)\n",
        "        plt.yticks(rotation=0, fontsize=8)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"✓ Confusion matrix saved to {save_path}\")\n",
        "\n",
        "    def generate_classification_report(self, preds, labels, save_path='results/classification_report.txt'):\n",
        "\n",
        "        # Create directory if it doesn't exist\n",
        "        output_dir = os.path.dirname(save_path)\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        report = classification_report(\n",
        "            labels,\n",
        "            preds,\n",
        "            target_names=self.class_names,\n",
        "            digits=4\n",
        "        )\n",
        "\n",
        "        print(\"\\n=== Classification Report ===\")\n",
        "        print(report)\n",
        "\n",
        "        # Save to file\n",
        "        with open(save_path, 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"✓ Classification report saved to {save_path}\")\n",
        "\n",
        "    def evaluate(self):\n",
        "\n",
        "        print(\"Evaluating model on test set...\")\n",
        "\n",
        "        preds, labels, probs = self.predict()\n",
        "        metrics = self.calculate_metrics(preds, labels)\n",
        "        self.plot_confusion_matrix(preds, labels)\n",
        "        self.generate_classification_report(preds, labels)\n",
        "\n",
        "        return metrics, preds, labels, probs\n",
        "\n",
        "# Usage\n",
        "test_df = pd.read_csv('data/splits/test.csv')\n",
        "\n",
        "# Ensure AtlasAugmentation is available and get the validation transform\n",
        "try:\n",
        "    augmentation = AtlasAugmentation()\n",
        "    val_transform = augmentation.get_val_transform()\n",
        "except NameError:\n",
        "    print(\"AtlasAugmentation class not found. Please ensure the cell defining it is executed.\")\n",
        "    exit() # Exit if the class is not available\n",
        "\n",
        "\n",
        "test_dataset = AtlasDataset(test_df, 'data/raw/images', transform=val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('models/checkpoints/best_resnet34.pth')\n",
        "model = ResNet34Classifier(num_classes=len(test_dataset.classes)) # Instantiate model before loading state_dict\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Define device if not already defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "evaluator = ModelEvaluator(model, test_loader, test_dataset.classes, device)\n",
        "metrics, preds, labels, probs = evaluator.evaluate()"
      ],
      "metadata": {
        "id": "0yELSKuRMttP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKC4nM0iroKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}